{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# A Notebook to analyze downloaded gridded climate time-series data \n",
    "\n",
    "## (Case study:  the Sauk-Suiattle Watershed )\n",
    "<img src= \"http://www.sauk-suiattle.com/images/Elliott.jpg\"\n",
    "style=\"float:left;width:150px;padding:20px\">   \n",
    "This data is compiled to digitally observe the Sauk-Suiattle Watershed, powered by HydroShare. <br />\n",
    "<br />\n",
    "Use this Jupyter Notebook to: <br />\n",
    "Migrate data sets from prior data download events,\n",
    "Compute daily, monthly, and annual temperature and precipitation statistics, <br /> \n",
    "Visualize precipitation results relative to the forcing data, <br />\n",
    "Visualize the time-series trends among the gridded cells using different Gridded data products. <br />\n",
    "\n",
    "<br /> <br /> <br /> <img src=\"https://www.washington.edu/brand/files/2014/09/W-Logo_Purple_Hex.png\" style=\"float:right;width:120px;padding:20px\">  \n",
    "#### A Watershed Dynamics Model by the Watershed Dynamics Research Group in the Civil and Environmental Engineering Department at the University of Washington "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  HydroShare Setup and Preparation\n",
    "\n",
    "To run this notebook, we must import several libaries. These are listed in order of 1) Python standard libraries, 2) hs_utils library provides functions for interacting with HydroShare, including resource querying, dowloading and creation, and 3) the observatory_gridded_hydromet library that is downloaded with this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge basemap-data-hires --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "import os\n",
    "import pandas as pd, numpy as np, dask, json\n",
    "import ogh\n",
    "import geopandas as gpd\n",
    "\n",
    "# data migration library\n",
    "from utilities import hydroshare\n",
    "\n",
    "# plotting and shape libraries\n",
    "%matplotlib inline\n",
    "\n",
    "# silencing warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# spatial plotting\n",
    "import fiona\n",
    "import shapely.ops\n",
    "from shapely.geometry import MultiPolygon, shape, point, box, Polygon\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# # spatial plotting\n",
    "# import fiona\n",
    "# import shapely.ops\n",
    "# from shapely.geometry import MultiPolygon, shape, point, box, Polygon\n",
    "# from descartes import PolygonPatch\n",
    "# from matplotlib.collections import PatchCollection\n",
    "# from mpl_toolkits.basemap import Basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize ogh_meta\n",
    "meta_file = dict(ogh.ogh_meta())\n",
    "sorted(meta_file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(meta_file['dailymet_livneh2013'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish a secure connection with HydroShare by instantiating the hydroshare class that is defined within hs_utils. In addition to connecting with HydroShare, this command also sets and prints environment variables for several parameters that will be useful for saving work back to HydroShare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "notebookdir = os.getcwd()\n",
    "\n",
    "hs=hydroshare.hydroshare()\n",
    "homedir = hs.getContentPath(os.environ[\"HS_RES_ID\"])\n",
    "os.chdir(homedir)\n",
    "print('Data will be loaded from and save to:'+homedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious about where the data is being downloaded, click on the Jupyter Notebook dashboard icon to return to the File System view.  The homedir directory location printed above is where you can find the data and contents you will download to a HydroShare JupyterHub server.  At the end of this work session, you can migrate this data to the HydroShare iRods server as a Generic Resource. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get list of gridded climate points for the watershed\n",
    "\n",
    "This example uses a shapefile with the watershed boundary of the Sauk-Suiattle Basin, which is stored in HydroShare at the following url: https://www.hydroshare.org/resource/c532e0578e974201a0bc40a37ef2d284/. \n",
    "\n",
    "The data for our processing routines can be retrieved using the getResourceFromHydroShare function by passing in the global identifier from the url above.  In the next cell, we download this resource from HydroShare, and identify that the points in this resource are available for downloading gridded hydrometeorology data, based on the point shapefile at https://www.hydroshare.org/resource/ef2d82bf960144b4bfb1bae6242bcc7f/, which is for the extent of North America and includes the average elevation for each 1/16 degree grid cell.  The file must include columns with station numbers, latitude, longitude, and elevation. The header of these columns must be FID, LAT, LONG_, and ELEV or RASTERVALU, respectively. The station numbers will be used for the remainder of the code to uniquely reference data from each climate station, as well as to identify minimum, maximum, and average elevation of all of the climate stations.  The webserice is currently set to a URL for the smallest geographic overlapping extent - e.g. WRF for Columbia River Basin (to use a limit using data from a FTP service, treatgeoself() would need to be edited in observatory_gridded_hydrometeorology utility). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sauk\n",
    "\"\"\"\n",
    "# Watershed extent\n",
    "hs.getResourceFromHydroShare('c532e0578e974201a0bc40a37ef2d284')\n",
    "sauk = hs.content['wbdhub12_17110006_WGS84_Basin.shp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the file availability from each watershed mapping file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the mappingfiles from usecase1\n",
    "mappingfile1 = os.path.join(homedir,'Sauk_mappingfile.csv')\n",
    "mappingfile2 = os.path.join(homedir,'Elwha_mappingfile.csv')\n",
    "mappingfile3 = os.path.join(homedir,'RioSalado_mappingfile.csv')\n",
    "\n",
    "t1 = ogh.mappingfileSummary(listofmappingfiles = [mappingfile1, mappingfile2, mappingfile3], \n",
    "                            listofwatershednames = ['Sauk-Suiattle river','Elwha river','Upper Rio Salado'],\n",
    "                            meta_file=meta_file)\n",
    "\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Compare Hydrometeorology \n",
    "\n",
    "This section performs computations and generates plots of the Livneh 2013, Livneh 2016, and WRF 2014 temperature and precipitation data in order to compare them with each other and observations. The generated plots are automatically downloaded and saved as .png files in the \"plots\" folder of the user's home directory and inline in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Livneh et al., 2013\n",
    "dr1 = meta_file['dailymet_livneh2013']\n",
    "\n",
    "# Salathe et al., 2014\n",
    "dr2 = meta_file['dailywrf_salathe2014']\n",
    "\n",
    "# define overlapping time window\n",
    "dr = ogh.overlappingDates(date_set1=tuple([dr1['start_date'], dr1['end_date']]), \n",
    "                          date_set2=tuple([dr2['start_date'], dr2['end_date']]))\n",
    "dr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INPUT: gridded meteorology from Jupyter Hub folders\n",
    "Data frames for each set of data are stored in a dictionary. The inputs to gridclim_dict() include the folder location and name of the hydrometeorology data, the file start and end, the analysis start and end, and the elevation band to be included in the analsyis (max and min elevation). <br/>  \n",
    "\n",
    "#### Create a dictionary of climate variables for the long-term mean (ltm) using the default elevation option of calculating a high, mid, and low elevation average.  The dictionary here is initialized with the Livneh et al., 2013 dataset with a dictionary output 'ltm_3bands', which is used as an input to the second time we run gridclim_dict(), to add the Salathe et al., 2014 data to the same dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ltm_3bands = ogh.gridclim_dict(mappingfile=mappingfile1,\n",
    "                               metadata=meta_file,\n",
    "                               dataset='dailymet_livneh2013',\n",
    "                               file_start_date=dr1['start_date'], \n",
    "                               file_end_date=dr1['end_date'],\n",
    "                               file_time_step=dr1['temporal_resolution'],\n",
    "                               subset_start_date=dr[0],\n",
    "                               subset_end_date=dr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ltm_3bands = ogh.gridclim_dict(mappingfile=mappingfile1,\n",
    "                               metadata=meta_file,\n",
    "                               dataset='dailyvic_livneh2013',\n",
    "                               file_start_date=dr1['start_date'], \n",
    "                               file_end_date=dr1['end_date'],\n",
    "                               file_time_step=dr1['temporal_resolution'],\n",
    "                               subset_start_date=dr[0],\n",
    "                               subset_end_date=dr[1],\n",
    "                               df_dict=ltm_3bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(ltm_3bands.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = meta_file['dailyvic_livneh2013']['variable_info']\n",
    "\n",
    "meta_df = pd.DataFrame.from_dict(meta).T\n",
    "\n",
    "meta_df.loc[['BASEFLOW','RUNOFF'],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltm_3bands['STREAMFLOW_dailyvic_livneh2013']=ltm_3bands['BASEFLOW_dailyvic_livneh2013']+ltm_3bands['RUNOFF_dailyvic_livneh2013']\n",
    "ltm_3bands['STREAMFLOW_dailyvic_livneh2013']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sauk-Suiattle\n",
    "\"\"\"\n",
    "# Watershed extent\n",
    "hs.getResourceFromHydroShare('c532e0578e974201a0bc40a37ef2d284')\n",
    "sauk = hs.content['wbdhub12_17110006_WGS84_Basin.shp']\n",
    "\n",
    "\"\"\"\n",
    "Elwha\n",
    "\"\"\"\n",
    "# Watershed extent\n",
    "hs.getResourceFromHydroShare('4aff8b10bc424250b3d7bac2188391e8', )\n",
    "elwha = hs.content[\"elwha_ws_bnd_wgs84.shp\"]\n",
    "\n",
    "\"\"\"\n",
    "Upper Rio Salado\n",
    "\"\"\"\n",
    "# Watershed extent\n",
    "hs.getResourceFromHydroShare('5c041d95ceb64dce8eb85d2a7db88ed7')\n",
    "riosalado = hs.content['UpperRioSalado_delineatedBoundary.shp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate surface area for each gridded cell\n",
    "def computegcSurfaceArea(shapefile, spatial_resolution, vardf):\n",
    "    \"\"\"\n",
    "    Data-driven computation of gridded cell surface area using the list of gridded cells centroids\n",
    "    \n",
    "    shapefile: (dir) the path to the study site shapefile for selecting the UTM boundary\n",
    "    spatial_resolution: (float) the spatial resolution in degree coordinate reference system e.g., 1/16\n",
    "    vardf: (dataframe) input dataframe that contains FID, LAT and LONG references for each gridded cell centroid\n",
    "    \n",
    "    return: (mean surface area in meters-squared, standard deviation in surface area)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ensure projection into WGS84 longlat values\n",
    "    ogh.reprojShapefile(shapefile)\n",
    "\n",
    "    # generate the figure axis\n",
    "    fig = plt.figure(figsize=(2,2), dpi=500)\n",
    "    ax1 = plt.subplot2grid((1,1),(0,0))\n",
    "\n",
    "    # calculate bounding box based on the watershed shapefile\n",
    "    watershed = gpd.read_file(shapefile)\n",
    "    watershed['watershed']='watershed'\n",
    "    watershed = watershed.dissolve(by='watershed')\n",
    "\n",
    "    # extract area centroid, bounding box info, and dimension shape\n",
    "    lon0, lat0 = np.array(watershed.centroid.iloc[0])\n",
    "    minx, miny, maxx, maxy = watershed.bounds.iloc[0]\n",
    "\n",
    "    # generate traverse mercatur projection\n",
    "    m = Basemap(projection='tmerc', resolution='h', ax=ax1, lat_0=lat0, lon_0=lon0,\n",
    "                llcrnrlon=minx, llcrnrlat=miny, urcrnrlon=maxx, urcrnrlat=maxy)\n",
    "\n",
    "    # generate gridded cell bounding boxes\n",
    "    midpt_dist=spatial_resolution/2\n",
    "    cat=vardf.T.reset_index(level=[1,2]).rename(columns={'level_1':'LAT','level_2':'LONG_'})\n",
    "    geometry = cat.apply(lambda x: \n",
    "                         shapely.ops.transform(m, box(x['LONG_']-midpt_dist, x['LAT']-midpt_dist,\n",
    "                                                      x['LONG_']+midpt_dist, x['LAT']+midpt_dist)), axis=1)\n",
    "\n",
    "    # compute gridded cell area\n",
    "    gc_area = geometry.apply(lambda x: x.area)\n",
    "    plt.gcf().clear()\n",
    "    return(gc_area.mean(), gc_area.sd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcSA = computeMeanSurfaceArea(shapefile=sauk, spatial_resolution=1/16, vardf=ltm_3bands['STREAMFLOW_dailyvic_livneh2013'])\n",
    "\n",
    "gcSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert mm/s to m/s\n",
    "df_dict = ltm_3bands\n",
    "objname = 'STREAMFLOW_dailyvic_livneh2013'\n",
    "dataset = objname.split('_',1)[1]\n",
    "gridcell_area = geo_area\n",
    "exceedance = 10\n",
    "\n",
    "\n",
    "\n",
    "# convert mmps to mps\n",
    "mmps = df_dict[objname]\n",
    "mps = mmps*0.001\n",
    "\n",
    "# multiply streamflow (mps) with grid cell surface area (m2) to produce volumetric streamflow (cms)\n",
    "cms = mps.multiply(np.array(geo_area))\n",
    "\n",
    "# convert m^3/s to cfs; multiply with (3.28084)^3\n",
    "cfs = cms.multiply((3.28084)**3)\n",
    "\n",
    "# output to df_dict\n",
    "df_dict['cfs_'+objname] = cfs\n",
    "\n",
    "# time-group by month-yearly streamflow volumetric values\n",
    "monthly_cfs = cfs.groupby(pd.TimeGrouper('M')).sum()\n",
    "monthly_cfs.index = pd.Series(monthly_cfs.index).apply(lambda x: x.strftime('%Y-%m'))\n",
    "\n",
    "# output to df_dict\n",
    "df_dict['monthly_cfs_'+objname] = monthly_cfs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# prepare for Exceedance computations\n",
    "row_indices = pd.Series(monthly_cfs.index).map(lambda x: pd.datetime.strptime(x, '%Y-%m').month)\n",
    "months = range(1,13)\n",
    "Exceed = pd.DataFrame()\n",
    "\n",
    "# for each month\n",
    "for eachmonth in months:\n",
    "    month_index = row_indices[row_indices==eachmonth].index\n",
    "    month_res = monthly_cfs.iloc[month_index,:].reset_index(drop=True)\n",
    "\n",
    "    # generate gridded-cell-specific 10% exceedance probability values\n",
    "    exceed = pd.DataFrame(month_res.apply(lambda x: np.percentile(x, 0.90), axis=0)).T\n",
    "    \n",
    "    # append to dataframe\n",
    "    Exceed = pd.concat([Exceed, exceed])\n",
    "    \n",
    "# set index to month order\n",
    "Exceed = Exceed.set_index(np.array(months))\n",
    "\n",
    "# output to df_dict\n",
    "df_dict['EXCEED{0}_{1}'.format(exceedance,dataset)] = Exceed\n",
    "\n",
    "# return(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthlyExceedence_cfs (df_dict,\n",
    "                           daily_streamflow_dfname,\n",
    "                           gridcell_area,\n",
    "                           exceedance=10,\n",
    "                           start_date=None,\n",
    "                           end_date=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    streamflow_df: (dataframe) streamflow values in cu. ft per second (row_index: date, col_index: gridcell ID)\n",
    "    daily_baseflow_df: (dataframe) groundwater flow in cu. ft per second (row_index: date, col_index: gridcell ID)\n",
    "    daily_surfacerunoff_df: (dateframe) surface flow in cu. ft per second (row_index: date, col_index: gridcell ID)\n",
    "    start_date: (datetime) start date\n",
    "    end_date: (datetime) end date\n",
    "    \"\"\"\n",
    "    \n",
    "    #daily_baseflow_df=None, #'BASEFLOW_dailyvic_livneh2013', \n",
    "    #daily_surfacerunoff_df=None, #'RUNOFF_dailyvic_livneh2013',\n",
    "\n",
    "    ## aggregate each daily streamflow value to a month-yearly sum\n",
    "    mmps = df_dict[daily_streamflow_dfname]\n",
    "    ## subset daily streamflow_df to that index range\n",
    "    if isinstance(start_date, type(None)):\n",
    "        startyear=0\n",
    "    if isinstance(end_date, type(None)):\n",
    "        endyear=len(mmps)-1\n",
    "    mmps = mmps.iloc[start_date:end_date,:]\n",
    "    \n",
    "    # convert mmps to mps\n",
    "    mps = mmps*0.001\n",
    "\n",
    "    # multiply streamflow (mps) with grid cell surface area (m2) to produce volumetric streamflow (cms)\n",
    "    cms = mps.multiply(np.array(geo_area))\n",
    "\n",
    "    # convert m^3/s to cfs; multiply with (3.28084)^3\n",
    "    cfs = cms.multiply((3.28084)**3)\n",
    "\n",
    "    # output to df_dict\n",
    "    df_dict['cfs_'+objname] = cfs\n",
    "\n",
    "    # time-group by month-yearly streamflow volumetric values\n",
    "    monthly_cfs = cfs.groupby(pd.TimeGrouper('M')).sum()\n",
    "    monthly_cfs.index = pd.Series(monthly_cfs.index).apply(lambda x: x.strftime('%Y-%m'))\n",
    "\n",
    "    # output to df_dict\n",
    "    df_dict['monthly_cfs_'+objname] = monthly_cfs\n",
    "\n",
    "    \n",
    "    monthly_streamflow_df = daily_streamflow_df.groupby(pd.TimeGrouper(\"M\")).sum()\n",
    "    \n",
    "    # loop through each station\n",
    "    \n",
    "    for eachcol in monthly_streamflow_df.columns():\n",
    "        station_moyr = dask.delayed(monthly_streamflow_df.loc[:,eachcol])\n",
    "        station_moyr\n",
    "        \n",
    "        \n",
    "df_dict = ltm_3bands\n",
    "objname = 'STREAMFLOW_dailyvic_livneh2013'\n",
    "dataset = objname.split('_',1)[1]\n",
    "gridcell_area = geo_area\n",
    "exceedance = 10\n",
    "\n",
    "ltm_3bands['STREAMFLOW_dailyvic_livneh2013']=ltm_3bands['BASEFLOW_dailyvic_livneh2013']+ltm_3bands['RUNOFF_dailyvic_livneh2013']\n",
    "        \n",
    "#         function [Qex] = monthlyExceedence_cfs(file,startyear,endyear)\n",
    "# % Load data from specified file\n",
    "# data = load(file);\n",
    "# Y=data(:,1);\n",
    "# MO=data(:,2);\n",
    "# D=data(:,3);\n",
    "# t = datenum(Y,MO,D);\n",
    "# %%%\n",
    "# % startyear=data(1,1);\n",
    "# % endyear=data(length(data),1);\n",
    "\n",
    "# Qnode = data(:,4);\n",
    "\n",
    "# %  Time control indices that cover selected period\n",
    "# d_start = datenum(startyear,10,01,23,0,0); % 1 hour early to catch record for that day\n",
    "# d_end = datenum(endyear,09,30,24,0,0); \n",
    "# idx = find(t>=d_start & t<=d_end);\n",
    "\n",
    "# exds=(1:19)./20;  % Exceedence probabilities from 0.05 to 0.95\n",
    "# mos=[10,11,12,1:9];\n",
    "# Qex(1:19,1:12)=0 ; % initialize array\n",
    "# for imo=1:12;\n",
    "#     mo=mos(imo);\n",
    "#     [Y, M, D, H, MI, S] = datevec(t(idx));\n",
    "#     ind=find(M==mo);  % find all flow values in that month in the period identified\n",
    "#     Q1=Qnode(idx(ind)); % input is in cfs\n",
    "#     for iex=1:19\n",
    "#         Qex(iex,imo)=quantile(Q1,1-exds(iex));\n",
    "#     end\n",
    "# end\n",
    "\n",
    "\n",
    "# end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltm_3bands['cfs_STREAMFLOW_dailyvic_livneh2013']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltm_3bands['monthly_cfs_STREAMFLOW_dailyvic_livneh2013']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltm_3bands['EXCEED10_dailyvic_livneh2013']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loop through each month to compute the 10% Exceedance Probability\n",
    "for eachmonth in range(1,13):\n",
    "    monthlabel = pd.datetime.strptime(str(eachmonth), '%m')\n",
    "\n",
    "    ogh.renderValuesInPoints(vardf=ltm_3bands['EXCEED10_dailyvic_livneh2013'],\n",
    "                             vardf_dateindex=eachmonth, \n",
    "                             shapefile=sauk.replace('.shp','_2.shp'),\n",
    "                             outfilepath='sauk{0}exceed10.png'.format(monthlabel.strftime('%b')), \n",
    "                             plottitle='Sauk {0} 10% Exceedance Probability'.format(monthlabel.strftime('%B')),\n",
    "                             colorbar_label='cubic feet per second',\n",
    "                             cmap='seismic_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize monthly precipitation spatially using Livneh et al., 2013 Meteorology data\n",
    "\n",
    "### Apply different plotting options:\n",
    "\n",
    "time-index option <br />\n",
    "Basemap option <br />\n",
    "colormap option <br />\n",
    "projection option <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "month=3\n",
    "monthlabel = pd.datetime.strptime(str(month), '%m')\n",
    "ogh.renderValuesInPoints(vardf=ltm_3bands['month_PRECIP_dailymet_livneh2013'],\n",
    "                         vardf_dateindex=month,\n",
    "                         shapefile=sauk.replace('.shp','_2.shp'), \n",
    "                         outfilepath=os.path.join(homedir, 'SaukPrecip{0}.png'.format(monthlabel.strftime('%b'))),\n",
    "                         plottitle='Sauk-Suiattle watershed'+'\\nPrecipitation in '+ monthlabel.strftime('%B'),\n",
    "                         colorbar_label='Average monthly precipitation (meters)',\n",
    "                         spatial_resolution=1/16, margin=0.5, epsg=3857,\n",
    "                         basemap_image='Demographics/USA_Social_Vulnerability_Index',\n",
    "                         cmap='gray_r')\n",
    "\n",
    "month=6\n",
    "monthlabel = pd.datetime.strptime(str(month), '%m')\n",
    "ogh.renderValuesInPoints(vardf=ltm_3bands['month_PRECIP_dailymet_livneh2013'],\n",
    "                         vardf_dateindex=month,\n",
    "                         shapefile=sauk.replace('.shp','_2.shp'), \n",
    "                         outfilepath=os.path.join(homedir, 'SaukPrecip{0}.png'.format(monthlabel.strftime('%b'))),\n",
    "                         plottitle='Sauk-Suiattle watershed'+'\\nPrecipitation in '+ monthlabel.strftime('%B'),\n",
    "                         colorbar_label='Average monthly precipitation (meters)',\n",
    "                         spatial_resolution=1/16, margin=0.5, epsg=3857,\n",
    "                         basemap_image='ESRI_StreetMap_World_2D',\n",
    "                         cmap='gray_r')\n",
    "\n",
    "month=9\n",
    "monthlabel = pd.datetime.strptime(str(month), '%m')\n",
    "ogh.renderValuesInPoints(vardf=ltm_3bands['month_PRECIP_dailymet_livneh2013'],\n",
    "                         vardf_dateindex=month,\n",
    "                         shapefile=sauk.replace('.shp','_2.shp'), \n",
    "                         outfilepath=os.path.join(homedir, 'SaukPrecip{0}.png'.format(monthlabel.strftime('%b'))),\n",
    "                         plottitle='Sauk-Suiattle watershed'+'\\nPrecipitation in '+ monthlabel.strftime('%B'),\n",
    "                         colorbar_label='Average monthly precipitation (meters)',\n",
    "                         spatial_resolution=1/16, margin=0.5, epsg=3857,\n",
    "                         basemap_image='ESRI_Imagery_World_2D',\n",
    "                         cmap='gray_r')\n",
    "\n",
    "month=12\n",
    "monthlabel = pd.datetime.strptime(str(month), '%m')\n",
    "ogh.renderValuesInPoints(vardf=ltm_3bands['month_PRECIP_dailymet_livneh2013'],\n",
    "                         vardf_dateindex=month,\n",
    "                         shapefile=sauk.replace('.shp','_2.shp'), \n",
    "                         outfilepath=os.path.join(homedir, 'SaukPrecip{0}.png'.format(monthlabel.strftime('%b'))),\n",
    "                         plottitle='Sauk-Suiattle watershed'+'\\nPrecipitation in '+ monthlabel.strftime('%B'),\n",
    "                         colorbar_label='Average monthly precipitation (meters)',\n",
    "                         spatial_resolution=1/16, margin=0.5, epsg=3857,\n",
    "                         basemap_image='Elevation/World_Hillshade',\n",
    "                         cmap='seismic_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize monthly precipitation difference between different gridded data products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for month in [3, 6, 9, 12]:\n",
    "    monthlabel = pd.datetime.strptime(str(month), '%m')\n",
    "    outfile='SaukLivnehPrecip{0}.png'.format(monthlabel.strftime('%b'))\n",
    "    \n",
    "    ax1 = ogh.renderValuesInPoints(vardf=ltm_3bands['month_PRECIP_dailymet_livneh2013'],\n",
    "                                   vardf_dateindex=month,\n",
    "                                   shapefile=sauk.replace('.shp','_2.shp'), \n",
    "                                   basemap_image='ESRI_Imagery_World_2D',\n",
    "                                   cmap='seismic_r',\n",
    "                                   plottitle='Sauk-Suiattle watershed'+'\\nPrecipitation in '+monthlabel.strftime('%B'),\n",
    "                                   colorbar_label='Average monthly precipitation (meters)',\n",
    "                                   outfilepath=os.path.join(homedir, outfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## comparison to WRF data from Salathe et al., 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ltm_3bands = ogh.gridclim_dict(mappingfile=mappingfile1,\n",
    "                               metadata=meta_file,\n",
    "                               dataset='dailywrf_salathe2014',\n",
    "                               colvar=None,\n",
    "                               file_start_date=dr2['start_date'], \n",
    "                               file_end_date=dr2['end_date'],\n",
    "                               file_time_step=dr2['temporal_resolution'],\n",
    "                               subset_start_date=dr[0],\n",
    "                               subset_end_date=dr[1],\n",
    "                               df_dict=ltm_3bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for month in [3, 6, 9, 12]:\n",
    "    monthlabel = pd.datetime.strptime(str(month), '%m')\n",
    "    outfile='SaukSalathePrecip{0}.png'.format(monthlabel.strftime('%b'))\n",
    "    \n",
    "    ax1 = ogh.renderValuesInPoints(vardf=ltm_3bands['month_PRECIP_dailywrf_salathe2014'],\n",
    "                                   vardf_dateindex=month,\n",
    "                                   shapefile=sauk.replace('.shp','_2.shp'), \n",
    "                                   basemap_image='ESRI_Imagery_World_2D',\n",
    "                                   cmap='seismic_r',\n",
    "                                   plottitle='Sauk-Suiattle watershed'+'\\nPrecipitation in '+monthlabel.strftime('%B'),\n",
    "                                   colorbar_label='Average monthly precipitation (meters)',\n",
    "                                   outfilepath=os.path.join(homedir, outfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_meanTmin(dictionary, loc_name, start_date, end_date):\n",
    "    # Plot 1: Monthly temperature analysis of Livneh data\n",
    "    if 'meanmonth_temp_min_liv2013_met_daily' and 'meanmonth_temp_min_wrf2014_met_daily' not in dictionary.keys():\n",
    "        pass\n",
    "\n",
    "    # generate month indices\n",
    "    wy_index=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "    wy_numbers=[10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    month_strings=[ 'Oct', 'Nov', 'Dec', 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sept']\n",
    "    \n",
    "    # initiate the plot object\n",
    "    fig, ax=plt.subplots(1,1,figsize=(10, 6))\n",
    "\n",
    "    if 'meanmonth_temp_min_liv2013_met_daily' in dictionary.keys():\n",
    "        # Liv2013\n",
    "\n",
    "        plt.plot(wy_index, dictionary['meanmonth_temp_min_liv2013_met_daily'][wy_numbers],'r-', linewidth=1, label='Liv Temp min')  \n",
    "    \n",
    "    if 'meanmonth_temp_min_wrf2014_met_daily' in dictionary.keys():\n",
    "        # WRF2014\n",
    "        plt.plot(wy_index, dictionary['meanmonth_temp_min_wrf2014_met_daily'][wy_numbers],'b-',linewidth=1, label='WRF Temp min')\n",
    " \n",
    "    if 'meanmonth_temp_min_livneh2013_wrf2014bc_met_daily' in dictionary.keys():\n",
    "        # WRF2014\n",
    "        plt.plot(wy_index, dictionary['meanmonth_temp_min_livneh2013_wrf2014bc_met_daily'][wy_numbers],'g-',linewidth=1, label='WRFbc Temp min')\n",
    " \n",
    "    # add reference line at y=0\n",
    "    plt.plot([1, 12],[0, 0], 'k-',linewidth=1)\n",
    "\n",
    "    plt.ylabel('Temp (C)',fontsize=14)\n",
    "    plt.xlabel('Month',fontsize=14)\n",
    "    plt.xlim(1,12);\n",
    "    plt.xticks(wy_index, month_strings);\n",
    "        \n",
    "    plt.tick_params(labelsize=12)\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(which='both')\n",
    "    plt.title(str(loc_name)+'\\nMinimum Temperature\\n Years: '+str(start_date.year)+'-'+str(end_date.year)+'; Elevation: '+str(dictionary['analysis_elev_min'])+'-'+str(dictionary['analysis_elev_max'])+'m', fontsize=16)\n",
    "    plt.savefig('monthly_Tmin'+str(loc_name)+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Compare gridded model to point observations\n",
    "\n",
    "### Read in  SNOTEL data - assess available data \n",
    "If you want to plot observed snotel point precipitation or temperature with the gridded climate data, set to 'Y' \n",
    "Give name of Snotel file and name to be used in figure legends. \n",
    "File format: Daily SNOTEL Data Report - Historic - By individual SNOTEL site, standard sensors (https://www.wcc.nrcs.usda.gov/snow/snotel-data.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sauk\n",
    "SNOTEL_file = os.path.join(homedir,'ThunderBasinSNOTEL.txt')\n",
    "SNOTEL_station_name='Thunder Creek'\n",
    "SNOTEL_file_use_colsnames = ['Date','Air Temperature Maximum (degF)', 'Air Temperature Minimum (degF)','Air Temperature Average (degF)','Precipitation Increment (in)']\n",
    "SNOTEL_station_elev=int(4320/3.281) # meters\n",
    "\n",
    "SNOTEL_obs_daily = ogh.read_daily_snotel(file_name=SNOTEL_file, \n",
    "                                         usecols=SNOTEL_file_use_colsnames,\n",
    "                                         delimiter=',', \n",
    "                                         header=58)\n",
    "\n",
    "# generate the start and stop date\n",
    "SNOTEL_obs_start_date=SNOTEL_obs_daily.index[0]\n",
    "SNOTEL_obs_end_date=SNOTEL_obs_daily.index[-1]\n",
    "\n",
    "# peek\n",
    "SNOTEL_obs_daily.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in  COOP station data - assess available data\n",
    "https://www.ncdc.noaa.gov/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "COOP_file=os.path.join(homedir, 'USC00455678.csv') # Sauk\n",
    "COOP_station_name='Mt Vernon'\n",
    "COOP_file_use_colsnames = ['DATE','PRCP','TMAX', 'TMIN','TOBS']\n",
    "COOP_station_elev=int(4.3) # meters\n",
    "\n",
    "COOP_obs_daily = ogh.read_daily_coop(file_name=COOP_file,\n",
    "                                     usecols=COOP_file_use_colsnames,\n",
    "                                     delimiter=',',\n",
    "                                     header=0)\n",
    "\n",
    "# generate the start and stop date\n",
    "COOP_obs_start_date=COOP_obs_daily.index[0]\n",
    "COOP_obs_end_date=COOP_obs_daily.index[-1]\n",
    "\n",
    "# peek\n",
    "COOP_obs_daily.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate new dictionary with original data\n",
    "ltm_0to3000 = ogh.gridclim_dict(metadata=meta_file,\n",
    "                                mappingfile=mappingfile1,\n",
    "                                dataset='dailymet_livneh2013',\n",
    "                                file_start_date=dr1['start_date'], \n",
    "                                file_end_date=dr1['end_date'], \n",
    "                                subset_start_date=dr[0],\n",
    "                                subset_end_date=dr[1])\n",
    "\n",
    "ltm_0to3000 = ogh.gridclim_dict(metadata=meta_file,\n",
    "                                mappingfile=mappingfile1,\n",
    "                                dataset='dailywrf_salathe2014',\n",
    "                                file_start_date=dr2['start_date'], \n",
    "                                file_end_date=dr2['end_date'], \n",
    "                                subset_start_date=dr[0],\n",
    "                                subset_end_date=dr[1],\n",
    "                                df_dict=ltm_0to3000)\n",
    "\n",
    "sorted(ltm_0to3000.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the mappingfile\n",
    "mappingfile = mappingfile1\n",
    "\n",
    "mapdf = pd.read_csv(mappingfile)\n",
    "\n",
    "# select station by first FID\n",
    "firstStation = ogh.findStationCode(mappingfile=mappingfile, colvar='FID', colvalue=0)\n",
    "\n",
    "# select station by elevation\n",
    "maxElevStation = ogh.findStationCode(mappingfile=mappingfile, colvar='ELEV', colvalue=mapdf.loc[:,'ELEV'].max())\n",
    "medElevStation = ogh.findStationCode(mappingfile=mappingfile, colvar='ELEV', colvalue=mapdf.loc[:,'ELEV'].median())\n",
    "minElevStation = ogh.findStationCode(mappingfile=mappingfile, colvar='ELEV', colvalue=mapdf.loc[:,'ELEV'].min())\n",
    "\n",
    "\n",
    "# print(firstStation, mapdf.iloc[0].ELEV)\n",
    "# print(maxElevStation, mapdf.loc[:,'ELEV'].max())\n",
    "# print(medElevStation, mapdf.loc[:,'ELEV'].median())\n",
    "# print(minElevStation, mapdf.loc[:,'ELEV'].min())\n",
    "\n",
    "# let's compare monthly averages for TMAX using livneh, salathe, and the salathe-corrected livneh\n",
    "comp = ['month_TMAX_dailymet_livneh2013',\n",
    "        'month_TMAX_dailywrf_salathe2014']\n",
    "\n",
    "obj = dict()\n",
    "for eachkey in ltm_0to3000.keys():\n",
    "    if eachkey in comp:\n",
    "        obj[eachkey] = ltm_0to3000[eachkey]\n",
    "\n",
    "panel_obj = pd.Panel.from_dict(obj)\n",
    "panel_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = ['meanmonth_TMAX_dailymet_livneh2013',\n",
    "        'meanmonth_TMAX_dailywrf_salathe2014']\n",
    "\n",
    "obj = dict()\n",
    "for eachkey in ltm_0to3000.keys():\n",
    "    if eachkey in comp:\n",
    "        obj[eachkey] = ltm_0to3000[eachkey]\n",
    "\n",
    "        df_obj = pd.DataFrame.from_dict(obj)\n",
    "df_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_res, var, dataset, pub = each.rsplit('_',3)\n",
    "\n",
    "print(t_res, var, dataset, pub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylab_var = meta_file['_'.join([dataset, pub])]['variable_info'][var]['desc']\n",
    "ylab_unit = meta_file['_'.join([dataset, pub])]['variable_info'][var]['units']\n",
    "\n",
    "print('{0} {1} ({2})'.format(t_res, ylab_var, ylab_unit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "comp = [['meanmonth_TMAX_dailymet_livneh2013','meanmonth_TMAX_dailywrf_salathe2014'],\n",
    "        ['meanmonth_PRECIP_dailymet_livneh2013','meanmonth_PRECIP_dailywrf_salathe2014']]\n",
    "wy_numbers=[10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "month_strings=[ 'Oct', 'Nov', 'Dec', 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep']\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,5), dpi=500)\n",
    "\n",
    "ax1 = plt.subplot2grid((2, 2), (0, 0), colspan=1)\n",
    "ax2 = plt.subplot2grid((2, 2), (1, 0), colspan=1)\n",
    "\n",
    "\n",
    "# monthly\n",
    "for eachsumm in df_obj.columns:\n",
    "    ax1.plot(df_obj[eachsumm])\n",
    "    \n",
    "\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=2, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obj[each].index.apply(lambda x: x+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "lws=[3, 10, 3, 3]\n",
    "styles=['b--','go-','y--','ro-']\n",
    "\n",
    "for col, style, lw in zip(comp, styles, lws):\n",
    "    panel_obj.xs(key=(minElevStation[0][0], minElevStation[0][1], minElevStation[0][2]), axis=2)[col].plot(style=style, lw=lw, ax=ax, legend=True)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=2)\n",
    "fig.show()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "lws=[3, 10, 3, 3]\n",
    "styles=['b--','go-','y--','ro-']\n",
    "\n",
    "for col, style, lw in zip(comp, styles, lws):\n",
    "    panel_obj.xs(key=(maxElevStation[0][0], maxElevStation[0][1], maxElevStation[0][2]), \n",
    "                 axis=2)[col].plot(style=style, lw=lw, ax=ax, legend=True)\n",
    "\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up VIC dictionary (as an example)  to compare to available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vic_dr1 = meta_file['dailyvic_livneh2013']['date_range']\n",
    "vic_dr2 = meta_file['dailyvic_livneh2015']['date_range']\n",
    "vic_dr = ogh.overlappingDates(tuple([vic_dr1['start'], vic_dr1['end']]),\n",
    "                              tuple([vic_dr2['start'], vic_dr2['end']]))\n",
    "\n",
    "vic_ltm_3bands = ogh.gridclim_dict(mappingfile=mappingfile,\n",
    "                                   metadata=meta_file,\n",
    "                                   dataset='dailyvic_livneh2013',\n",
    "                                   file_start_date=vic_dr1['start'], \n",
    "                                   file_end_date=vic_dr1['end'],\n",
    "                                   file_time_step=vic_dr1['time_step'],\n",
    "                                   subset_start_date=vic_dr[0],\n",
    "                                   subset_end_date=vic_dr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(vic_ltm_3bands.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save the results back into HydroShare\n",
    "<a name=\"creation\"></a>\n",
    "\n",
    "Using the `hs_utils` library, the results of the Geoprocessing steps above can be saved back into HydroShare.  First, define all of the required metadata for resource creation, i.e. *title*, *abstract*, *keywords*, *content files*.  In addition, we must define the type of resource that will be created, in this case *genericresource*.  \n",
    "\n",
    "***Note:*** Make sure you save the notebook at this point, so that all notebook changes will be saved into the new HydroShare resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute this cell to list the content of the directory\n",
    "!ls -lt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of files to save to HydroShare. Verify location and names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -zcf {climate2013_tar} livneh2013\n",
    "!tar -zcf {climate2015_tar} livneh2015\n",
    "!tar -zcf {wrf_tar} salathe2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ThisNotebook='Observatory_Sauk_TreatGeoSelf.ipynb' #check name for consistency\n",
    "climate2013_tar = 'livneh2013.tar.gz'\n",
    "climate2015_tar = 'livneh2015.tar.gz'\n",
    "wrf_tar = 'salathe2014.tar.gz'\n",
    "mappingfile = 'Sauk_mappingfile.csv'\n",
    "\n",
    "files=[ThisNotebook, mappingfile, climate2013_tar, climate2015_tar, wrf_tar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each file downloaded onto the server folder, move to a new HydroShare Generic Resource\n",
    "title = 'Results from testing out the TreatGeoSelf utility'\n",
    "abstract = 'This the output from the TreatGeoSelf utility integration notebook.'\n",
    "keywords = ['Sauk', 'climate', 'Landlab','hydromet','watershed'] \n",
    "rtype = 'genericresource'  \n",
    "\n",
    "# create the new resource\n",
    "resource_id = hs.createHydroShareResource(abstract, \n",
    "                                          title,\n",
    "                                          keywords=keywords, \n",
    "                                          resource_type=rtype, \n",
    "                                          content_files=files, \n",
    "                                          public=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
